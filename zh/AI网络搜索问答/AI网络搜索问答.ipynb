{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d67f3e2-0af5-4b55-af9b-28bde3561ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "import re\n",
    "import requests, time, logging, json\n",
    "import uuid\n",
    "import tldextract\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "load_dotenv(\"config.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_openai_response(messages,response_format = \"text\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                   \"role\": \"user\",\n",
    "                   \"content\": [\n",
    "                       {\n",
    "                           \"type\": \"text\",\n",
    "                           \"text\": messages\n",
    "                       }\n",
    "                   ]\n",
    "                }\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": response_format\n",
    "            }\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"调用OpenAI API出错: {e}\")\n",
    "        return None\n",
    "\n",
    "class Website_agent:\n",
    "    def __init__(self, domain, existing_urls=None):\n",
    "        self.crawl_key = os.getenv(\"CRAWLBASE_KEY\")\n",
    "        if not existing_urls:\n",
    "            self.existing_urls = [domain]\n",
    "        else:\n",
    "            self.existing_urls = existing_urls\n",
    "\n",
    "        self.main_domain = domain\n",
    "        if not domain.startswith(('http://', 'https://')):\n",
    "            domain = 'https://' + domain\n",
    "        self.domain = re.sub(r'/$', '', domain)\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def generate_request_id(self):\n",
    "        random_uuid = uuid.uuid4()\n",
    "        return random_uuid\n",
    "\n",
    "    def fetch_html_by_js_token(self, url, max_retry=2):\n",
    "        api_url = f'https://api.crawlbase.com/?token={self.crawl_key}&url={url}'\n",
    "        for i in range(max_retry):\n",
    "            try:\n",
    "                print(f\"正在抓取页面: {url} (第{i + 1}次尝试)\")\n",
    "                response = requests.get(api_url)\n",
    "                if response.status_code == 200:\n",
    "                    print(\"页面抓取成功\")\n",
    "                    return response.text\n",
    "                else:\n",
    "                    print(f\"页面请求失败，状态码: {response.status_code}，尝试重试...\")\n",
    "                    if i < max_retry - 1:\n",
    "                        time.sleep(1)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"网络请求异常: {str(e)}，尝试重试...\")\n",
    "                if i < max_retry - 1:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    raise\n",
    "        raise requests.exceptions.RequestException(f\"达到最大重试次数，URL抓取失败: {url}\")\n",
    "\n",
    "    def extract_head_info(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        head_tag = soup.head\n",
    "\n",
    "        title = head_tag.title.string if head_tag.title else None\n",
    "\n",
    "        description = head_tag.find('meta', attrs={'name': 'description'}).get('content') if head_tag.find('meta',\n",
    "                                                                                                           attrs={\n",
    "                                                                                                               'name': 'description'}) else None\n",
    "\n",
    "        return title, description\n",
    "\n",
    "    def extract_body_tags(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        body_tag = soup.body\n",
    "\n",
    "        if not body_tag:\n",
    "            return False\n",
    "\n",
    "        for script_tag in body_tag.find_all('script'):\n",
    "            script_tag.extract()\n",
    "\n",
    "        for noscript_tag in body_tag.find_all('noscript'):\n",
    "            noscript_tag.extract()\n",
    "\n",
    "        for style_tag in body_tag.find_all('style'):\n",
    "            style_tag.extract()\n",
    "\n",
    "        for link_tag in body_tag.find_all('link'):\n",
    "            link_tag.extract()\n",
    "\n",
    "        for img_tag in body_tag.find_all('img'):\n",
    "            img_tag.attrs = {'alt': img_tag.get('alt', '')}\n",
    "\n",
    "        for tag in body_tag.find_all(True):\n",
    "            tag.attrs = {attr: value for attr, value in tag.attrs.items() if attr.lower() != 'style'}\n",
    "\n",
    "        for svg_tag in body_tag.find_all('svg'):\n",
    "            svg_tag.extract()\n",
    "\n",
    "        for tag in body_tag.find_all(True):\n",
    "            if tag.name == 'a':\n",
    "                tag.attrs = {attr: value for attr, value in tag.attrs.items() if attr.lower() == 'href'}\n",
    "            else:\n",
    "                tag.attrs = {}\n",
    "\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "\n",
    "        body_tags = ''.join(str(tag) for tag in body_tag.contents)\n",
    "        body_tags = body_tags.replace(\"\\n\", \"\")\n",
    "\n",
    "        return body_tags\n",
    "\n",
    "    def extract_and_process_values(self, text):\n",
    "        status_pattern = r'<status>(.*?)</status>'\n",
    "        url_pattern = r'<url>(.*?)</url>'\n",
    "        answer_pattern = r'<answer>(.*?)</answer>'\n",
    "\n",
    "        status_matches = re.findall(status_pattern, text, re.DOTALL)\n",
    "        url_matches = re.findall(url_pattern, text)\n",
    "        answer_matches = re.findall(answer_pattern, text, re.DOTALL)\n",
    "\n",
    "        statuses = [match.strip() for match in status_matches]\n",
    "        urls = []\n",
    "        contacts = []\n",
    "        for match in url_matches:\n",
    "            match = match.strip()\n",
    "            if match:\n",
    "                if match.startswith('tel:') or match.startswith('mailto:'):\n",
    "                    contacts.append(match)\n",
    "                else:\n",
    "                    if not match.startswith('http://') and not match.startswith('https://'):\n",
    "                        if match.startswith('/'):\n",
    "                            url_value = self.domain.rstrip('/') + match\n",
    "                        else:\n",
    "                            url_value = match\n",
    "                    else:\n",
    "                        url_value = match\n",
    "                    url_value = re.sub(r'/$', '', url_value)\n",
    "                    if url_value not in self.existing_urls:\n",
    "                        match_domain = extract_main_domain(url_value)\n",
    "                        if match_domain == self.main_domain:\n",
    "                            urls.append(url_value)\n",
    "                        else:\n",
    "                            print(f\"跳过外部域名 - 当前域名:{self.main_domain}, 匹配域名:{match_domain}\")\n",
    "\n",
    "        urls = list(set(urls))\n",
    "        answers = [match.strip() for match in answer_matches]\n",
    "\n",
    "        result = {\n",
    "            'status': 0,\n",
    "            'answer': '',\n",
    "            'urls': [],\n",
    "            'contacts': []\n",
    "        }\n",
    "\n",
    "        if 'Fully meets the query requirement' in statuses:\n",
    "            result['status'] = 1\n",
    "        elif 'Partially meets the query requirement' in statuses:\n",
    "            result['status'] = 2\n",
    "        elif 'Unable to meet the query requirement' in statuses:\n",
    "            result['status'] = 3\n",
    "\n",
    "        if answers:\n",
    "            result['answer'] = ' '.join(answers)\n",
    "\n",
    "        if urls:\n",
    "            result['urls'] = urls\n",
    "\n",
    "        if contacts:\n",
    "            result['contacts'] = contacts\n",
    "\n",
    "        return result\n",
    "\n",
    "    def process_website(self, htmlObj, url, question):\n",
    "        print(f\"\\n开始处理网页: {url}\")\n",
    "        if 'body' not in htmlObj or not htmlObj['body']:\n",
    "            return {\"status\": 4, \"answer\": \"\", \"urls\": [], \"error\": \"页面内容为空\"}\n",
    "\n",
    "        if 'title' not in htmlObj:\n",
    "            htmlObj['title'] = ''\n",
    "        if 'description' not in htmlObj:\n",
    "            htmlObj['description'] = ''\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"###\n",
    "            The URL to be crawled:{url}\n",
    "            Title:{htmlObj['title']}\n",
    "            Description:{htmlObj['description']}\n",
    "            Information contained in the website's HTML body tags:\n",
    "            {htmlObj['body']}\n",
    "            ###\n",
    "\n",
    "            Instructions:\n",
    "            1. The content enclosed by ### above describes website content. When responding to a Question, provide answers based on this website content.\n",
    "            2. There are three statuses for responding to questions (the provided URL is just an example, and the answer tag should only contain the response):\n",
    "               a). <status>Fully meets the query requirement</status> <answer>The answer</answer>\n",
    "               b). <status>Partially meets the query requirement</status>, with the potential for more detailed answers by crawling <urls><url>website1</url><url>website2</url></urls>, <answer>The answer</answer>\n",
    "               c). <status>Unable to meet the query requirement</status>, if there are recommended URLs to crawl, please specify them in the format: <urls><url>website address</url><url>website address</url></urls> for the answer\n",
    "            3. The suggested URLs for crawling should not be identical to the URL being crawled: {url}.\n",
    "            4. When the question requires providing contact information, include email, phone number, social media, etc.\n",
    "            5. If there is data in the website content that is relevant to the question, it's best to reflect that relevant data in the answer.    \n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "            \"\"\"\n",
    "            print(\"正在分析页面内容...\")\n",
    "            response = get_openai_response(prompt)\n",
    "            print(\"页面分析完成，开始提取结果...\")\n",
    "            result = self.extract_and_process_values(response)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"处理页面时出现错误: {str(e)}\")\n",
    "            return {\"status\": 0, \"answer\": \"\", \"urls\": [], \"error\": str(e)}\n",
    "\n",
    "    def fetch_html_and_process_website(self, url, question):\n",
    "        try:\n",
    "            print(f\"\\n获取并处理页面: {url}\")\n",
    "            html_content = self.fetch_html_by_js_token(url)\n",
    "            title, description = self.extract_head_info(html_content)\n",
    "            body_tags = self.extract_body_tags(html_content)\n",
    "\n",
    "            if body_tags is False:\n",
    "                return {\"status\": 4, \"error\": \"页面内容为空\"}\n",
    "\n",
    "            prompt = f\"\"\"###\n",
    "            The URL to be crawled:{url}\n",
    "            Title:{title}\n",
    "            Description:{description}\n",
    "            Information contained in the website's HTML body tags:\n",
    "            {body_tags}\n",
    "            ###\n",
    "\n",
    "            Instructions:\n",
    "            1. The content enclosed by ### above describes website content. When responding to a Question, provide answers based on this website content.\n",
    "            2. There are three statuses for responding to questions (the provided URL is just an example, and the answer tag should only contain the response):\n",
    "               a). <status>Fully meets the query requirement</status> <answer>The answer</answer>\n",
    "               b). <status>Partially meets the query requirement</status>, with the potential for more detailed answers by crawling <urls><url>website1</url><url>website2</url></urls>, <answer>The answer</answer>\n",
    "               c). <status>Unable to meet the query requirement</status>, if there are recommended URLs to crawl, please specify them in the format: <urls><url>website address</url><url>website address</url></urls> for the answer\n",
    "            3. The suggested URLs for crawling should not be identical to the URL being crawled: {url}.\n",
    "            4. When the question requires providing contact information, include email, phone number, social media, etc.\n",
    "            5. If there is data in the website content that is relevant to the question, it's best to reflect that relevant data in the answer.    \n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "            \"\"\"\n",
    "\n",
    "            response = get_openai_response(prompt)\n",
    "            result = self.extract_and_process_values(response)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"处理页面时出现错误: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_website_url(self, htmlObj, url, question, request_id=None):\n",
    "        print(f\"\\n提取页面URL: {url}\")\n",
    "        if 'body' not in htmlObj or not htmlObj['body']:\n",
    "            return {\"status\": 4, \"error\": \"页面内容为空\"}\n",
    "\n",
    "        if 'title' not in htmlObj:\n",
    "            htmlObj['title'] = ''\n",
    "        if 'description' not in htmlObj:\n",
    "            htmlObj['description'] = ''\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"###\n",
    "            The URL to be crawled:{url}\n",
    "            Title:{htmlObj['title']}\n",
    "            Description:{htmlObj['description']}\n",
    "            Information contained in the website's HTML body tags:\n",
    "            {htmlObj['body']}\n",
    "            ###\n",
    "            你是一名数据分析工程师。\n",
    "\n",
    "            问题：{question}\n",
    "\n",
    "            1.分析html并且提取所有有效网址以及描述信息\n",
    "            2.根据提取的信息把与问题相关的网址分别列出来，并使用<url></url>标签标记出来。\n",
    "            3.不相关的不需要标记。\n",
    "            \"\"\"\n",
    "\n",
    "            response = get_openai_response(prompt)\n",
    "            url_pattern = r'<url>(.*?)</url>'\n",
    "            url_matches = re.findall(url_pattern, response)\n",
    "            urls = []\n",
    "            contacts = []\n",
    "            for match in url_matches:\n",
    "                match = match.strip()\n",
    "                url_value = \"\"\n",
    "                if match:\n",
    "                    if match.startswith('tel:') or match.startswith('mailto:'):\n",
    "                        contacts.append(match)\n",
    "                    else:\n",
    "                        if not match.startswith('http://') and not match.startswith('https://'):\n",
    "                            if match.startswith('/'):\n",
    "                                url_value = self.domain.rstrip('/') + match\n",
    "                        else:\n",
    "                            match_domain = extract_main_domain(match)\n",
    "                            if match_domain == self.main_domain:\n",
    "                                url_value = match\n",
    "                            else:\n",
    "                                print(f\"跳过外部域名 - 当前域名:{self.main_domain}, 匹配域名:{match_domain}\")\n",
    "                        if url_value:\n",
    "                            url_value = re.sub(r'/$', '', url_value)\n",
    "                            if url_value not in self.existing_urls:\n",
    "                                urls.append(url_value)\n",
    "\n",
    "            urls = list(set(urls))\n",
    "\n",
    "            if not request_id:\n",
    "                request_id = self.generate_request_id()\n",
    "\n",
    "            return urls\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"处理页面URL时出现错误: {str(e)}\")\n",
    "            return {\"status\": 0, \"error\": str(e)}\n",
    "\n",
    "    def test_process_website(self, url, question, max_depth=3):\n",
    "        request_id = self.generate_request_id()\n",
    "        print(f\"\\n=== 开始网站分析 ===\")\n",
    "        print(f\"初始URL: {url}\")\n",
    "        print(f\"最大探索深度: {max_depth}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"\\n第1步: 获取页面内容\")\n",
    "            html_content = self.fetch_html_by_js_token(url)\n",
    "            title, description = self.extract_head_info(html_content)\n",
    "            body_tags = self.extract_body_tags(html_content)\n",
    "\n",
    "            if not body_tags:\n",
    "                print(\"错误: 页面内容为空\")\n",
    "                return {\"request_id\": request_id, \"status\": 4, \"error\": \"页面内容为空\"}\n",
    "\n",
    "            print(\"\\n第2步: 分析页面内容\")\n",
    "            htmlObj = {'title': title, 'description': description, 'body': body_tags}\n",
    "            result = self.process_website(htmlObj, url, question)\n",
    "            \n",
    "            print(f\"\\n第3步: 处理分析结果\")\n",
    "            print(f\"状态码: {result['status']}\")\n",
    "            if result['status'] == 1:\n",
    "                print(\"✓ 已找到完整答案\")\n",
    "                return {'status': result['status'], 'answer': result['answer']}\n",
    "            \n",
    "            elif result['status'] == 2:\n",
    "                print(\"→ 找到部分答案，需要进一步分析...\")\n",
    "                if max_depth > 0:\n",
    "                    print(\"\\n第4步: 分析相关页面\")\n",
    "                    return self._process_additional_urls(result, htmlObj, url, question, request_id, max_depth)\n",
    "                else:\n",
    "                    print(\"已达到最大深度，返回当前结果\")\n",
    "                    return {'status': 2, 'answer': result['answer']}\n",
    "                    \n",
    "            elif result['status'] == 3:\n",
    "                print(\"→ 未找到答案，尝试分析其他页面...\")\n",
    "                if max_depth > 0:\n",
    "                    print(\"\\n第4步: 分析其他相关页面\")\n",
    "                    return self._process_additional_urls(result, htmlObj, url, question, request_id, max_depth)\n",
    "                else:\n",
    "                    print(\"已达到最大深度，无法找到答案\")\n",
    "                    return {'status': 3}\n",
    "            \n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n处理过程中出现错误: {str(e)}\")\n",
    "            return {\"status\": 0, \"error\": str(e)}\n",
    "\n",
    "    def _process_additional_urls(self, initial_result, htmlObj, url, question, request_id, max_depth):\n",
    "        urls = initial_result['urls'] or self.process_website_url(htmlObj, url, question, request_id)\n",
    "        if urls:\n",
    "            print(f\"发现 {len(urls)} 个相关链接\")\n",
    "        answers = [initial_result['answer']] if initial_result['answer'] else []\n",
    "        \n",
    "        for i, url_value in enumerate(urls, 1):\n",
    "            if url_value not in self.existing_urls:\n",
    "                print(f\"\\n分析相关链接 {i}/{len(urls)}: {url_value}\")\n",
    "                self.existing_urls.append(url_value)\n",
    "                sub_result = self.fetch_html_and_process_website(url_value, question)\n",
    "                \n",
    "                if sub_result['status'] == 1:\n",
    "                    print(\"✓ 在相关页面中找到完整答案\")\n",
    "                    return sub_result\n",
    "                elif sub_result['status'] == 2:\n",
    "                    print(\"→ 找到部分相关信息\")\n",
    "                    answers.append(sub_result['answer'])\n",
    "\n",
    "        if answers:\n",
    "            print(\"\\n整合所有找到的信息...\")\n",
    "            return {'status': 2, 'answer': '\\n'.join(answers)}\n",
    "        \n",
    "        print(\"\\n未能找到相关答案\")\n",
    "        return {'status': 3}\n",
    "\n",
    "def extract_main_domain(url):\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'https://' + url\n",
    "    ext = tldextract.extract(url)\n",
    "    return \".\".join(part for part in (ext.domain, ext.suffix) if part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3615c0c-c0db-4c93-8318-4a23bf6e6e38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 开始网站分析 ===\n",
      "初始URL: https://www.percent.cn\n",
      "最大探索深度: 3\n",
      "\n",
      "第1步: 获取页面内容\n",
      "正在抓取页面: https://www.percent.cn (第1次尝试)\n",
      "页面抓取成功\n",
      "\n",
      "第2步: 分析页面内容\n",
      "\n",
      "开始处理网页: https://www.percent.cn\n",
      "正在分析页面内容...\n",
      "页面分析完成，开始提取结果...\n",
      "跳过外部域名 - 当前域名:percent.cn, 匹配域名:linkedin.com\n",
      "跳过外部域名 - 当前域名:percent.cn, 匹配域名:crunchbase.com\n",
      "\n",
      "第3步: 处理分析结果\n",
      "状态码: 2\n",
      "→ 找到部分答案，需要进一步分析...\n",
      "\n",
      "第4步: 分析相关页面\n",
      "\n",
      "提取页面URL: https://www.percent.cn\n",
      "发现 1 个相关链接\n",
      "\n",
      "分析相关链接 1/1: https://percent.cn/Contact.html\n",
      "\n",
      "获取并处理页面: https://percent.cn/Contact.html\n",
      "正在抓取页面: https://percent.cn/Contact.html (第1次尝试)\n",
      "页面抓取成功\n",
      "→ 找到部分相关信息\n",
      "\n",
      "整合所有找到的信息...\n",
      "抓取结果:\n",
      "状态码: 2\n",
      "答案: 目前提供的内容没有涉及主要成员的信息。不过，您可以通过以下方式联系百分点科技：电话：400-6240-800，商务邮箱：business@percent.cn。\n",
      "联系方式包括：\n",
      "- 咨询电话：400-6240-800\n",
      "- 商务合作邮箱：business@percent.cn\n",
      "- 人才招聘邮箱：hr@percent.cn\n",
      "- 媒体合作邮箱：pr@percent.cn\n",
      "主要成员信息未在提供的网站内容中列出。\n"
     ]
    }
   ],
   "source": [
    "# 测试代码\n",
    "crawl_url = \"https://www.percent.cn\"\n",
    "existing_urls = [crawl_url]\n",
    "main_domain = extract_main_domain(crawl_url)\n",
    "question = \"主要成员以及联系方式\"\n",
    "\n",
    "website_bot = Website_agent(main_domain,existing_urls)\n",
    "result = website_bot.test_process_website(crawl_url,question)\n",
    "\n",
    "print(\"抓取结果:\")\n",
    "print(f\"状态码: {result['status']}\")\n",
    "\n",
    "if result['status'] == 0:\n",
    "    print(f\"错误信息:\")\n",
    "else:\n",
    "    if result['status'] == 2 or result['status'] == 1:\n",
    "        print(f\"答案: {result['answer']}\")\n",
    "    if result['status'] == 3:\n",
    "        print(\"无法回答\")\n",
    "    if result['status'] == 4:\n",
    "        print(\"无法回答\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf0790cd-b135-4695-b431-2857a0adbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "serper_api_key = os.getenv(\"SERPER_KEY\")\n",
    "\n",
    "def process_query(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    分析用户查询并执行相应的技能\n",
    "    Args:\n",
    "        query: 用户的查询内容\n",
    "    Returns:\n",
    "        执行结果的字典\n",
    "    \"\"\"\n",
    "    print(f\"\\n开始处理用户查询: {query}\")\n",
    "    \n",
    "    prompt = f\"\"\"你是AI网络搜索问答助手，你的主要工作是根据问题来判断下一步工作应该触发哪个技能。\n",
    "\n",
    "技能分类：\n",
    "搜索官方网站 (type_code: search)\n",
    "使用场景：当用户询问某个产品、公司或服务，但未提供官方URL\n",
    "必须生成用于查找官方网站的搜索关键词\n",
    "返回格式：{{\"type_code\": \"search\", \"keyword\": \"公司名称 official website\",\"question\":\"用户的问题\"}}\n",
    "\n",
    "网站采集 (type_code: crawl)\n",
    "使用场景：用户直接提供了目标URL\n",
    "返回格式：{{\"type_code\": \"crawl\", \"url\": \"用户提供的URL\",\"question\":\"用户的问题\"}}\n",
    "\n",
    "直接回答 (type_code: other)\n",
    "使用场景：普通问答或闲聊\n",
    "返回格式：{{\"type_code\": \"other\", \"answer\": \"回答内容\"}}\n",
    "\n",
    "严格按照上述JSON格式返回结果，不要包含任何其他说明文字。\n",
    "\n",
    "用户问题：{query}\n",
    "\"\"\"\n",
    "\n",
    "    # 获取GPT分析结果\n",
    "    print(\"正在分析查询类型...\")\n",
    "    analysis_result = json.loads(get_openai_response(prompt, \"json_object\"))\n",
    "    print(f\"分析结果: {json.dumps(analysis_result, ensure_ascii=False, indent=2)}\")\n",
    "\n",
    "    # 根据分析结果执行相应技能\n",
    "    if analysis_result['type_code'] == 'search':\n",
    "        print(f\"\\n执行搜索技能\")\n",
    "        print(f\"搜索关键词: {analysis_result['keyword']}\")\n",
    "        return search_and_process(analysis_result['question'])\n",
    "            \n",
    "    elif analysis_result['type_code'] == 'crawl':\n",
    "        print(f\"\\n执行网站采集\")\n",
    "        crawl_url = analysis_result['url']\n",
    "        existing_urls = [crawl_url]\n",
    "        main_domain = extract_main_domain(crawl_url)\n",
    "        website_bot = Website_agent(main_domain, existing_urls)\n",
    "        result = website_bot.test_process_website(crawl_url, analysis_result['question'])\n",
    "        \n",
    "        print(\"抓取结果:\")\n",
    "        print(f\"状态码: {result['status']}\")\n",
    "        if result['status'] == 0:\n",
    "            print(f\"错误信息:\")\n",
    "        else:\n",
    "            if result['status'] == 2 or result['status'] == 1:\n",
    "                print(f\"答案: {result['answer']}\")\n",
    "            if result['status'] == 3:\n",
    "                print(\"无法回答\")\n",
    "            if result['status'] == 4:\n",
    "                print(\"无法回答\")\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    else:  # type_code == 'other'\n",
    "        print(\"\\n执行直接回答\")\n",
    "        return analysis_result\n",
    "\n",
    "def build_official_site_prompt(search_results: list) -> str:\n",
    "    \"\"\"\n",
    "    构建用于判断官方网站的prompt\n",
    "    Args:\n",
    "        search_results: Google搜索结果列表\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 构建官网识别Prompt ===\")\n",
    "    \n",
    "    prompt = \"\"\"你是一个网站分类专家。你的任务是从搜索结果中识别出最可能的官方网站。\n",
    "    \n",
    "规则：\n",
    "1. 官方网站通常具有以下特征：\n",
    "   - 域名通常是公司/品牌名称\n",
    "   - URL结构简单，通常是根域名\n",
    "   - 网站标题通常包含公司/品牌名称\n",
    "   - 一般不会是社交媒体、新闻网站或第三方平台的链接\n",
    "\n",
    "2. 需要考虑的因素（按重要性排序）：\n",
    "   a) 域名与公司/品牌名称的匹配度\n",
    "   b) 是否是品牌的主域名（而不是子域名）\n",
    "   c) 链接的可信度\n",
    "   d) 网站描述的官方性\n",
    "\n",
    "请分析以下搜索结果，并以JSON格式返回最可能的官方网站URL。\n",
    "\n",
    "搜索结果：\n",
    "\"\"\"\n",
    "    # 添加搜索结果到prompt\n",
    "    print(f\"\\n分析以下搜索结果：\")\n",
    "    for idx, result in enumerate(search_results, 1):\n",
    "        url = result.get('link', '')\n",
    "        title = result.get('title', '')\n",
    "        snippet = result.get('snippet', '')\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "{idx}. 网站信息：\n",
    "   URL: {url}\n",
    "   标题: {title}\n",
    "   描述: {snippet}\n",
    "\"\"\"\n",
    "        print(f\"\\n{idx}. {url}\")\n",
    "        print(f\"   标题: {title}\")\n",
    "\n",
    "    prompt += \"\"\"\n",
    "请以以下JSON格式返回分析结果：\n",
    "{\n",
    "    \"official_url\": \"最可能的官方网站URL\",\n",
    "    \"confidence\": \"高/中/低\",\n",
    "    \"reason\": \"简要说明选择原因\"\n",
    "}\n",
    "\n",
    "注意：\n",
    "- 如果没有找到明显的官方网站，confidence设为\"低\"\n",
    "- 如果发现多个可能的官方网站，选择最可能的一个\n",
    "\"\"\"\n",
    "    print(\"\\nPrompt构建完成\")\n",
    "    return prompt\n",
    "\n",
    "def process_search_results(search_results: dict, question: str) -> dict:\n",
    "    \"\"\"\n",
    "    处理Google搜索结果，找出官方网站\n",
    "    Args:\n",
    "        search_results: Google Serper的完整搜索结果\n",
    "        question: 原始问题\n",
    "    Returns:\n",
    "        dict: 包含官方网站URL的结果\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 开始处理搜索结果 ===\")\n",
    "    try:\n",
    "        if 'organic' not in search_results:\n",
    "            print(\"❌ 错误：搜索结果格式错误\")\n",
    "            return {\"error\": \"搜索结果格式错误\"}\n",
    "\n",
    "        print(f\"找到 {len(search_results['organic'])} 个搜索结果\")\n",
    "        \n",
    "        # 构建分析prompt\n",
    "        print(\"\\n第1步：构建分析Prompt\")\n",
    "        prompt = build_official_site_prompt(search_results['organic'][:5])  # 只分析前5个结果\n",
    "        \n",
    "        # 分析结果\n",
    "        print(\"\\n第2步：使用AI分析搜索结果\")\n",
    "        try:\n",
    "            result = json.loads(get_openai_response(prompt, \"json_object\"))\n",
    "            \n",
    "            print(f\"\\n=== AI分析结果 ===\")\n",
    "            print(f\"✓ 识别出的官方网站: {result['official_url']}\")\n",
    "            print(f\"✓ 置信度: {result['confidence']}\")\n",
    "            print(f\"✓ 选择原因: {result['reason']}\")\n",
    "            \n",
    "            if result['official_url']:\n",
    "                if result['confidence'] == \"低\":\n",
    "                    print(\"\\n警告：官网识别置信度较低，但仍继续处理\")\n",
    "                    \n",
    "                print(f\"\\n第3步：开始网站采集\")\n",
    "                print(f\"目标网址: {result['official_url']}\")\n",
    "                \n",
    "                # 实例化Website_agent并执行采集\n",
    "                crawl_url = result['official_url']\n",
    "                existing_urls = [crawl_url]\n",
    "                main_domain = extract_main_domain(crawl_url)\n",
    "                \n",
    "                agent = Website_agent(main_domain,existing_urls)\n",
    "                return agent.test_process_website(\n",
    "                    url=crawl_url,\n",
    "                    question=question\n",
    "                )\n",
    "            else:\n",
    "                print(\"\\n❌ 错误：未找到可信的官方网站\")\n",
    "                return {\"error\": \"未找到可信的官方网站\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ 错误：分析过程失败\")\n",
    "            print(f\"错误信息: {str(e)}\")\n",
    "            return {\"error\": f\"分析过程发生错误: {str(e)}\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 错误：处理搜索结果失败\")\n",
    "        print(f\"错误信息: {str(e)}\")\n",
    "        return {\"error\": f\"处理搜索结果时发生错误: {str(e)}\"}\n",
    "\n",
    "def search_and_process(query: str) -> dict:\n",
    "    \"\"\"搜索并处理结果\"\"\"\n",
    "    print(\"\\n=== 开始搜索流程 ===\")\n",
    "    search_keyword = f\"{query.strip()} official website\"\n",
    "    print(f\"搜索关键词: {search_keyword}\")\n",
    "    \n",
    "    headers = {\n",
    "        'X-API-KEY': serper_api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'q': search_keyword,\n",
    "        'gl': 'us',\n",
    "        'page': 0,\n",
    "        'num': 5\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n第1步：调用搜索API\")\n",
    "        response = requests.post(\n",
    "            \"https://google.serper.dev/search\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ 搜索请求成功\")\n",
    "            search_results = response.json()\n",
    "            \n",
    "            print(\"\\n第2步：处理搜索结果\")\n",
    "            return process_search_results(search_results, query)\n",
    "        else:\n",
    "            print(f\"\\n❌ 错误：搜索请求失败 (状态码: {response.status_code})\")\n",
    "            return {\"error\": f\"搜索请求失败: {response.status_code}\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 错误：搜索过程失败\")\n",
    "        print(f\"错误信息: {str(e)}\")\n",
    "        return {\"error\": f\"搜索过程发生错误: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82ed1b0a-ee7a-4138-a956-f8b20915d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始处理用户查询: https://www.percent.cn/百分点公司主要成员以及联系方式\n",
      "正在分析查询类型...\n",
      "分析结果: {\n",
      "  \"type_code\": \"crawl\",\n",
      "  \"url\": \"https://www.percent.cn/\",\n",
      "  \"question\": \"百分点公司主要成员以及联系方式\"\n",
      "}\n",
      "\n",
      "执行网站采集\n",
      "\n",
      "=== 开始网站分析 ===\n",
      "初始URL: https://www.percent.cn/\n",
      "最大探索深度: 3\n",
      "\n",
      "第1步: 获取页面内容\n",
      "正在抓取页面: https://www.percent.cn/ (第1次尝试)\n",
      "页面抓取成功\n",
      "\n",
      "第2步: 分析页面内容\n",
      "\n",
      "开始处理网页: https://www.percent.cn/\n",
      "正在分析页面内容...\n",
      "页面分析完成，开始提取结果...\n",
      "\n",
      "第3步: 处理分析结果\n",
      "状态码: 2\n",
      "→ 找到部分答案，需要进一步分析...\n",
      "\n",
      "第4步: 分析相关页面\n",
      "\n",
      "提取页面URL: https://www.percent.cn/\n",
      "发现 2 个相关链接\n",
      "\n",
      "分析相关链接 1/2: https://www.percent.cn/Company.html\n",
      "\n",
      "获取并处理页面: https://www.percent.cn/Company.html\n",
      "正在抓取页面: https://www.percent.cn/Company.html (第1次尝试)\n",
      "页面抓取成功\n",
      "→ 找到部分相关信息\n",
      "\n",
      "分析相关链接 2/2: https://www.percent.cn/Contact.html\n",
      "\n",
      "获取并处理页面: https://www.percent.cn/Contact.html\n",
      "正在抓取页面: https://www.percent.cn/Contact.html (第1次尝试)\n",
      "页面抓取成功\n",
      "→ 找到部分相关信息\n",
      "\n",
      "整合所有找到的信息...\n",
      "抓取结果:\n",
      "状态码: 2\n",
      "答案: 目前从所提供的内容中无法获取百分点公司的主要成员信息。不过，您可以联系他们获取更多信息。以下是他们的联系方式：\n",
      "- 咨询电话：400-6240-800\n",
      "- 商务邮箱：business@percent.cn\n",
      "- 更多联系方式可以访问他们的联系页面：<a href=\"https://www.percent.cn/Contact.html\">更多联系方式</a>\n",
      "The content does not provide detailed information about the main members of 百分点科技公司 (Percent Company). However, for contact information, the consultation phone number is 400-6240-800, and the business email is business@percent.cn. For more contact details, you can visit their contact page: <a href=\"https://www.percent.cn/Contact.html\">Contact Us</a>.\n",
      "该网站页面提供了百分点公司的联系方式，但未提供公司主要成员的信息。联系方式如下：\n",
      "- 咨询电话: 400-6240-800\n",
      "- 商务邮箱: business@percent.cn\n",
      "- 人才招聘: hr@percent.cn\n",
      "- 媒体合作: pr@percent.cn\n",
      "\n",
      "办公地点：\n",
      "- 北京（总部）：北京市海淀区建材城中路27号金隅智造工场S5栋西侧百分点办公区\n",
      "- 上海：上海市浦东新区上钢新村街道友诚路149号SK大厦20层01-03单元\n",
      "- 沈阳：沈阳市沈河区青年大街1-1号沈阳市府恒隆广场办公楼1座18层\n",
      "- 广州（天河区）：广州市天河区花城大道85号高德置地春广场A座2606B\n",
      "- 广州（白云区）：广州市白云区云城东路565号宏鼎云璟汇主楼2408室\n",
      "查询问题：https://www.percent.cn/百分点公司主要成员以及联系方式\n",
      "\n",
      "最终结果：\n",
      "{'status': 2, 'answer': '目前从所提供的内容中无法获取百分点公司的主要成员信息。不过，您可以联系他们获取更多信息。以下是他们的联系方式：\\n- 咨询电话：400-6240-800\\n- 商务邮箱：business@percent.cn\\n- 更多联系方式可以访问他们的联系页面：<a href=\"https://www.percent.cn/Contact.html\">更多联系方式</a>\\nThe content does not provide detailed information about the main members of 百分点科技公司 (Percent Company). However, for contact information, the consultation phone number is 400-6240-800, and the business email is business@percent.cn. For more contact details, you can visit their contact page: <a href=\"https://www.percent.cn/Contact.html\">Contact Us</a>.\\n该网站页面提供了百分点公司的联系方式，但未提供公司主要成员的信息。联系方式如下：\\n- 咨询电话: 400-6240-800\\n- 商务邮箱: business@percent.cn\\n- 人才招聘: hr@percent.cn\\n- 媒体合作: pr@percent.cn\\n\\n办公地点：\\n- 北京（总部）：北京市海淀区建材城中路27号金隅智造工场S5栋西侧百分点办公区\\n- 上海：上海市浦东新区上钢新村街道友诚路149号SK大厦20层01-03单元\\n- 沈阳：沈阳市沈河区青年大街1-1号沈阳市府恒隆广场办公楼1座18层\\n- 广州（天河区）：广州市天河区花城大道85号高德置地春广场A座2606B\\n- 广州（白云区）：广州市白云区云城东路565号宏鼎云璟汇主楼2408室'}\n"
     ]
    }
   ],
   "source": [
    "#result2 = search_and_process(query2)\n",
    "\n",
    "query2 = \"https://www.percent.cn/百分点公司主要成员以及联系方式\"\n",
    "#query2 = \"百分点公司主要成员以及联系方式\"\n",
    "#query2 = \"今天天气怎么样\"\n",
    "result2 = process_query(query2)\n",
    "print(f\"查询问题：{query2}\")\n",
    "print(\"\\n最终结果：\")\n",
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
