{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0251a70-9fa8-4e74-bda2-a991b783784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "import re\n",
    "import requests, time, logging, json\n",
    "import uuid\n",
    "import tldextract\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "load_dotenv(\"config.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_openai_response(messages, response_format=\"text\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": messages\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": response_format\n",
    "            }\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return None\n",
    "\n",
    "class Website_agent:\n",
    "    def __init__(self, domain, existing_urls=None):\n",
    "        self.crawl_key = os.getenv(\"CRAWLBASE_KEY\")\n",
    "        if not existing_urls:\n",
    "            self.existing_urls = [domain]\n",
    "        else:\n",
    "            self.existing_urls = existing_urls\n",
    "\n",
    "        self.main_domain = domain\n",
    "        if not domain.startswith(('http://', 'https://')):\n",
    "            domain = 'https://' + domain\n",
    "        self.domain = re.sub(r'/$', '', domain)\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def generate_request_id(self):\n",
    "        random_uuid = uuid.uuid4()\n",
    "        return random_uuid\n",
    "\n",
    "    def fetch_html_by_js_token(self, url, max_retry=2):\n",
    "        api_url = f'https://api.crawlbase.com/?token={self.crawl_key}&url={url}'\n",
    "        for i in range(max_retry):\n",
    "            try:\n",
    "                print(f\"Fetching page: {url} (Attempt {i + 1})\")\n",
    "                response = requests.get(api_url)\n",
    "                if response.status_code == 200:\n",
    "                    print(\"Page successfully fetched\")\n",
    "                    return response.text\n",
    "                else:\n",
    "                    print(f\"Request failed, status code: {response.status_code}, retrying...\")\n",
    "                    if i < max_retry - 1:\n",
    "                        time.sleep(1)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Network request error: {str(e)}, retrying...\")\n",
    "                if i < max_retry - 1:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    raise\n",
    "        raise requests.exceptions.RequestException(f\"Max retries exceeded, URL fetching failed: {url}\")\n",
    "\n",
    "    def extract_head_info(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        head_tag = soup.head\n",
    "\n",
    "        title = head_tag.title.string if head_tag.title else None\n",
    "\n",
    "        description = head_tag.find('meta', attrs={'name': 'description'}).get('content') if head_tag.find('meta',\n",
    "                                                                                                           attrs={\n",
    "                                                                                                               'name': 'description'}) else None\n",
    "\n",
    "        return title, description\n",
    "\n",
    "    def extract_body_tags(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        body_tag = soup.body\n",
    "\n",
    "        if not body_tag:\n",
    "            return False\n",
    "\n",
    "        for script_tag in body_tag.find_all('script'):\n",
    "            script_tag.extract()\n",
    "\n",
    "        for noscript_tag in body_tag.find_all('noscript'):\n",
    "            noscript_tag.extract()\n",
    "\n",
    "        for style_tag in body_tag.find_all('style'):\n",
    "            style_tag.extract()\n",
    "\n",
    "        for link_tag in body_tag.find_all('link'):\n",
    "            link_tag.extract()\n",
    "\n",
    "        for img_tag in body_tag.find_all('img'):\n",
    "            img_tag.attrs = {'alt': img_tag.get('alt', '')}\n",
    "\n",
    "        for tag in body_tag.find_all(True):\n",
    "            tag.attrs = {attr: value for attr, value in tag.attrs.items() if attr.lower() != 'style'}\n",
    "\n",
    "        for svg_tag in body_tag.find_all('svg'):\n",
    "            svg_tag.extract()\n",
    "\n",
    "        for tag in body_tag.find_all(True):\n",
    "            if tag.name == 'a':\n",
    "                tag.attrs = {attr: value for attr, value in tag.attrs.items() if attr.lower() == 'href'}\n",
    "            else:\n",
    "                tag.attrs = {}\n",
    "\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "\n",
    "        body_tags = ''.join(str(tag) for tag in body_tag.contents)\n",
    "        body_tags = body_tags.replace(\"\\n\", \"\")\n",
    "\n",
    "        return body_tags\n",
    "\n",
    "    def extract_and_process_values(self, text):\n",
    "        status_pattern = r'<status>(.*?)</status>'\n",
    "        url_pattern = r'<url>(.*?)</url>'\n",
    "        answer_pattern = r'<answer>(.*?)</answer>'\n",
    "\n",
    "        status_matches = re.findall(status_pattern, text, re.DOTALL)\n",
    "        url_matches = re.findall(url_pattern, text)\n",
    "        answer_matches = re.findall(answer_pattern, text, re.DOTALL)\n",
    "\n",
    "        statuses = [match.strip() for match in status_matches]\n",
    "        urls = []\n",
    "        contacts = []\n",
    "        for match in url_matches:\n",
    "            match = match.strip()\n",
    "            if match:\n",
    "                if match.startswith('tel:') or match.startswith('mailto:'):\n",
    "                    contacts.append(match)\n",
    "                else:\n",
    "                    if not match.startswith('http://') and not match.startswith('https://'):\n",
    "                        if match.startswith('/'):\n",
    "                            url_value = self.domain.rstrip('/') + match\n",
    "                        else:\n",
    "                            url_value = match\n",
    "                    else:\n",
    "                        url_value = match\n",
    "                    url_value = re.sub(r'/$', '', url_value)\n",
    "                    if url_value not in self.existing_urls:\n",
    "                        match_domain = extract_main_domain(url_value)\n",
    "                        if match_domain == self.main_domain:\n",
    "                            urls.append(url_value)\n",
    "                        else:\n",
    "                            print(f\"Skipping external domain - Current domain:{self.main_domain}, Matched domain:{match_domain}\")\n",
    "\n",
    "        urls = list(set(urls))\n",
    "        answers = [match.strip() for match in answer_matches]\n",
    "\n",
    "        result = {\n",
    "            'status': 0,\n",
    "            'answer': '',\n",
    "            'urls': [],\n",
    "            'contacts': []\n",
    "        }\n",
    "\n",
    "        if 'Fully meets the query requirement' in statuses:\n",
    "            result['status'] = 1\n",
    "        elif 'Partially meets the query requirement' in statuses:\n",
    "            result['status'] = 2\n",
    "        elif 'Unable to meet the query requirement' in statuses:\n",
    "            result['status'] = 3\n",
    "\n",
    "        if answers:\n",
    "            result['answer'] = ' '.join(answers)\n",
    "\n",
    "        if urls:\n",
    "            result['urls'] = urls\n",
    "\n",
    "        if contacts:\n",
    "            result['contacts'] = contacts\n",
    "\n",
    "        return result\n",
    "\n",
    "    def process_website(self, htmlObj, url, question):\n",
    "        print(f\"\\nProcessing website: {url}\")\n",
    "        if 'body' not in htmlObj or not htmlObj['body']:\n",
    "            return {\"status\": 4, \"answer\": \"\", \"urls\": [], \"error\": \"Page content is empty\"}\n",
    "\n",
    "        if 'title' not in htmlObj:\n",
    "            htmlObj['title'] = ''\n",
    "        if 'description' not in htmlObj:\n",
    "            htmlObj['description'] = ''\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"###\n",
    "            The URL to be crawled:{url}\n",
    "            Title:{htmlObj['title']}\n",
    "            Description:{htmlObj['description']}\n",
    "            Information contained in the website's HTML body tags:\n",
    "            {htmlObj['body']}\n",
    "            ###\n",
    "\n",
    "            Instructions:\n",
    "            1. The content enclosed by ### above describes website content. When responding to a Question, provide answers based on this website content.\n",
    "            2. There are three statuses for responding to questions (the provided URL is just an example, and the answer tag should only contain the response):\n",
    "               a). <status>Fully meets the query requirement</status> <answer>The answer</answer>\n",
    "               b). <status>Partially meets the query requirement</status>, with the potential for more detailed answers by crawling <urls><url>website1</url><url>website2</url></urls>, <answer>The answer</answer>\n",
    "               c). <status>Unable to meet the query requirement</status>, if there are recommended URLs to crawl, please specify them in the format: <urls><url>website address</url><url>website address</url></urls> for the answer\n",
    "            3. The suggested URLs for crawling should not be identical to the URL being crawled: {url}.\n",
    "            4. When the question requires providing contact information, include email, phone number, social media, etc.\n",
    "            5. If there is data in the website content that is relevant to the question, it's best to reflect that relevant data in the answer.    \n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "            \"\"\"\n",
    "            print(\"Analyzing page content...\")\n",
    "            response = get_openai_response(prompt)\n",
    "            print(\"Page analysis completed, extracting results...\")\n",
    "            result = self.extract_and_process_values(response)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing website: {str(e)}\")\n",
    "            return {\"status\": 0, \"answer\": \"\", \"urls\": [], \"error\": str(e)}\n",
    "\n",
    "    def fetch_html_and_process_website(self, url, question):\n",
    "        try:\n",
    "            print(f\"\\nFetching and processing page: {url}\")\n",
    "            html_content = self.fetch_html_by_js_token(url)\n",
    "            title, description = self.extract_head_info(html_content)\n",
    "            body_tags = self.extract_body_tags(html_content)\n",
    "\n",
    "            if body_tags is False:\n",
    "                return {\"status\": 4, \"error\": \"Page content is empty\"}\n",
    "\n",
    "            prompt = f\"\"\"###\n",
    "            The URL to be crawled:{url}\n",
    "            Title:{title}\n",
    "            Description:{description}\n",
    "            Information contained in the website's HTML body tags:\n",
    "            {body_tags}\n",
    "            ###\n",
    "\n",
    "            Instructions:\n",
    "            1. The content enclosed by ### above describes website content. When responding to a Question, provide answers based on this website content.\n",
    "            2. There are three statuses for responding to questions (the provided URL is just an example, and the answer tag should only contain the response):\n",
    "               a). <status>Fully meets the query requirement</status> <answer>The answer</answer>\n",
    "               b). <status>Partially meets the query requirement</status>, with the potential for more detailed answers by crawling <urls><url>website1</url><url>website2</url></urls>, <answer>The answer</answer>\n",
    "               c). <status>Unable to meet the query requirement</status>, if there are recommended URLs to crawl, please specify them in the format: <urls><url>website address</url><url>website address</url></urls> for the answer\n",
    "            3. The suggested URLs for crawling should not be identical to the URL being crawled: {url}.\n",
    "            4. When the question requires providing contact information, include email, phone number, social media, etc.\n",
    "            5. If there is data in the website content that is relevant to the question, it's best to reflect that relevant data in the answer.    \n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "            \"\"\"\n",
    "\n",
    "            response = get_openai_response(prompt)\n",
    "            result = self.extract_and_process_values(response)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing website: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_website_url(self, htmlObj, url, question, request_id=None):\n",
    "        print(f\"\\nExtracting page URL: {url}\")\n",
    "        if 'body' not in htmlObj or not htmlObj['body']:\n",
    "            return {\"status\": 4, \"error\": \"Page content is empty\"}\n",
    "\n",
    "        if 'title' not in htmlObj:\n",
    "            htmlObj['title'] = ''\n",
    "        if 'description' not in htmlObj:\n",
    "            htmlObj['description'] = ''\n",
    "\n",
    "        try:\n",
    "            prompt = f\"\"\"###\n",
    "            The URL to be crawled:{url}\n",
    "            Title:{htmlObj['title']}\n",
    "            Description:{htmlObj['description']}\n",
    "            Information contained in the website's HTML body tags:\n",
    "            {htmlObj['body']}\n",
    "            ###\n",
    "            You are a data analysis engineer.\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            1. Analyze the HTML and extract all valid URLs and description information.\n",
    "            2. Based on the extracted information, list URLs related to the question using <url></url> tags.\n",
    "            3. Irrelevant ones should not be tagged.\n",
    "            \"\"\"\n",
    "\n",
    "            response = get_openai_response(prompt)\n",
    "            url_pattern = r'<url>(.*?)</url>'\n",
    "            url_matches = re.findall(url_pattern, response)\n",
    "            urls = []\n",
    "            contacts = []\n",
    "            for match in url_matches:\n",
    "                match = match.strip()\n",
    "                url_value = \"\"\n",
    "                if match:\n",
    "                    if match.startswith('tel:') or match.startswith('mailto:'):\n",
    "                        contacts.append(match)\n",
    "                    else:\n",
    "                        if not match.startswith('http://') and not match.startswith('https://'):\n",
    "                            if match.startswith('/'):\n",
    "                                url_value = self.domain.rstrip('/') + match\n",
    "                        else:\n",
    "                            match_domain = extract_main_domain(match)\n",
    "                            if match_domain == self.main_domain:\n",
    "                                url_value = match\n",
    "                            else:\n",
    "                                print(f\"Skipping external domain - Current domain:{self.main_domain}, Matched domain:{match_domain}\")\n",
    "                        if url_value:\n",
    "                            url_value = re.sub(r'/$', '', url_value)\n",
    "                            if url_value not in self.existing_urls:\n",
    "                                urls.append(url_value)\n",
    "\n",
    "            urls = list(set(urls))\n",
    "\n",
    "            if not request_id:\n",
    "                request_id = self.generate_request_id()\n",
    "\n",
    "            return urls\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page URL: {str(e)}\")\n",
    "            return {\"status\": 0, \"error\": str(e)}\n",
    "\n",
    "    def test_process_website(self, url, question, max_depth=3):\n",
    "        request_id = self.generate_request_id()\n",
    "        print(f\"\\n=== Starting website analysis ===\")\n",
    "        print(f\"Initial URL: {url}\")\n",
    "        print(f\"Max exploration depth: {max_depth}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nStep 1: Fetching page content\")\n",
    "            html_content = self.fetch_html_by_js_token(url)\n",
    "            title, description = self.extract_head_info(html_content)\n",
    "            body_tags = self.extract_body_tags(html_content)\n",
    "\n",
    "            if not body_tags:\n",
    "                print(\"Error: Page content is empty\")\n",
    "                return {\"request_id\": request_id, \"status\": 4, \"error\": \"Page content is empty\"}\n",
    "\n",
    "            print(\"\\nStep 2: Analyzing page content\")\n",
    "            htmlObj = {'title': title, 'description': description, 'body': body_tags}\n",
    "            result = self.process_website(htmlObj, url, question)\n",
    "            \n",
    "            print(f\"\\nStep 3: Processing analysis result\")\n",
    "            print(f\"Status code: {result['status']}\")\n",
    "            if result['status'] == 1:\n",
    "                print(\"✓ Complete answer found\")\n",
    "                return {'status': result['status'], 'answer': result['answer']}\n",
    "            \n",
    "            elif result['status'] == 2:\n",
    "                print(\"→ Partial answer found, further analysis needed...\")\n",
    "                if max_depth > 0:\n",
    "                    print(\"\\nStep 4: Analyzing related pages\")\n",
    "                    return self._process_additional_urls(result, htmlObj, url, question, request_id, max_depth)\n",
    "                else:\n",
    "                    print(\"Max depth reached, returning current results\")\n",
    "                    return {'status': 2, 'answer': result['answer']}\n",
    "                    \n",
    "            elif result['status'] == 3:\n",
    "                print(\"→ No answer found, trying other pages...\")\n",
    "                if max_depth > 0:\n",
    "                    print(\"\\nStep 4: Analyzing other related pages\")\n",
    "                    return self._process_additional_urls(result, htmlObj, url, question, request_id, max_depth)\n",
    "                else:\n",
    "                    print(\"Max depth reached, unable to find answer\")\n",
    "                    return {'status': 3}\n",
    "            \n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during processing: {str(e)}\")\n",
    "            return {\"status\": 0, \"error\": str(e)}\n",
    "\n",
    "    def _process_additional_urls(self, initial_result, htmlObj, url, question, request_id, max_depth):\n",
    "        urls = initial_result['urls'] or self.process_website_url(htmlObj, url, question, request_id)\n",
    "        if urls:\n",
    "            print(f\"Found {len(urls)} related links\")\n",
    "        answers = [initial_result['answer']] if initial_result['answer'] else []\n",
    "        \n",
    "        for i, url_value in enumerate(urls, 1):\n",
    "            if url_value not in self.existing_urls:\n",
    "                print(f\"\\nAnalyzing related link {i}/{len(urls)}: {url_value}\")\n",
    "                self.existing_urls.append(url_value)\n",
    "                sub_result = self.fetch_html_and_process_website(url_value, question)\n",
    "                \n",
    "                if sub_result['status'] == 1:\n",
    "                    print(\"✓ Complete answer found in related page\")\n",
    "                    return sub_result\n",
    "                elif sub_result['status'] == 2:\n",
    "                    print(\"→ Partial related information found\")\n",
    "                    answers.append(sub_result['answer'])\n",
    "\n",
    "        if answers:\n",
    "            print(\"\\nConsolidating all found information...\")\n",
    "            return {'status': 2, 'answer': '\\n'.join(answers)}\n",
    "        \n",
    "        print(\"\\nNo related answers found\")\n",
    "        return {'status': 3}\n",
    "\n",
    "def extract_main_domain(url):\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'https://' + url\n",
    "    ext = tldextract.extract(url)\n",
    "    return \".\".join(part for part in (ext.domain, ext.suffix) if part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29bd388-0163-433d-b64d-75a70f9e5cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting website analysis ===\n",
      "Initial URL: https://www.percent.cn\n",
      "Max exploration depth: 3\n",
      "\n",
      "Step 1: Fetching page content\n",
      "Fetching page: https://www.percent.cn (Attempt 1)\n",
      "Page successfully fetched\n",
      "\n",
      "Step 2: Analyzing page content\n",
      "\n",
      "Processing website: https://www.percent.cn\n",
      "Analyzing page content...\n",
      "Page analysis completed, extracting results...\n",
      "\n",
      "Step 3: Processing analysis result\n",
      "Status code: 2\n",
      "→ Partial answer found, further analysis needed...\n",
      "\n",
      "Step 4: Analyzing related pages\n",
      "\n",
      "Extracting page URL: https://www.percent.cn\n",
      "Found 2 related links\n",
      "\n",
      "Analyzing related link 1/2: https://percent.cn/Contact.html\n",
      "\n",
      "Fetching and processing page: https://percent.cn/Contact.html\n",
      "Fetching page: https://percent.cn/Contact.html (Attempt 1)\n",
      "Page successfully fetched\n",
      "→ Partial related information found\n",
      "\n",
      "Analyzing related link 2/2: https://percent.cn/Company.html\n",
      "\n",
      "Fetching and processing page: https://percent.cn/Company.html\n",
      "Fetching page: https://percent.cn/Company.html (Attempt 1)\n",
      "Page successfully fetched\n",
      "\n",
      "Consolidating all found information...\n",
      "Crawl result:\n",
      "Status code: 2\n",
      "Answer: The contact information provided on the website is a phone number and an email address. The phone number for inquiries is 400-6240-800, and the business email is business@percent.cn. Additional contact details may be available on their contact page: [Contact Information](https://www.percent.cn/Contact.html). However, there is no information regarding main members of the organization on the provided website content.\n",
      "Contact information for 百分点科技 includes:\n",
      "- General Inquiry Phone: 400-6240-800\n",
      "- Business Email: business@percent.cn\n",
      "- HR Email: hr@percent.cn\n",
      "- Media Email: pr@percent.cn\n",
      "\n",
      "The question about main members is not answered with the available content.\n"
     ]
    }
   ],
   "source": [
    "# Testing code\n",
    "crawl_url = \"https://www.percent.cn\"\n",
    "existing_urls = [crawl_url]\n",
    "main_domain = extract_main_domain(crawl_url)\n",
    "question = \"Main members and contact information\"\n",
    "\n",
    "website_bot = Website_agent(main_domain, existing_urls)\n",
    "result = website_bot.test_process_website(crawl_url, question)\n",
    "\n",
    "print(\"Crawl result:\")\n",
    "print(f\"Status code: {result['status']}\")\n",
    "\n",
    "if result['status'] == 0:\n",
    "    print(\"Error information:\")\n",
    "else:\n",
    "    if result['status'] == 2 or result['status'] == 1:\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "    if result['status'] == 3:\n",
    "        print(\"Unable to provide an answer\")\n",
    "    if result['status'] == 4:\n",
    "        print(\"Unable to provide an answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3f06ec-a129-49b9-9735-8ddcae511dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "serper_api_key = os.getenv(\"SERPER_KEY\")\n",
    "\n",
    "def process_query(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the user's query and execute the corresponding skill\n",
    "    Args:\n",
    "        query: The user's query content\n",
    "    Returns:\n",
    "        A dictionary with the execution result\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting query processing: {query}\")\n",
    "    \n",
    "    prompt = f\"\"\"You are an AI web search Q&A assistant, your main job is to determine which skill to trigger next based on the question.\n",
    "\n",
    "Skill Types:\n",
    "Search Official Website (type_code: search)\n",
    "Use Case: When the user inquires about a product, company, or service but does not provide the official URL.\n",
    "You must generate a search keyword for finding the official website.\n",
    "Return Format: {{\"type_code\": \"search\", \"keyword\": \"company name official website\",\"question\":\"user's question\"}}\n",
    "\n",
    "Website Crawling (type_code: crawl)\n",
    "Use Case: The user directly provides the target URL.\n",
    "Return Format: {{\"type_code\": \"crawl\", \"url\": \"user-provided URL\",\"question\":\"user's question\"}}\n",
    "\n",
    "Direct Answer (type_code: other)\n",
    "Use Case: For general Q&A or chit-chat.\n",
    "Return Format: {{\"type_code\": \"other\", \"answer\": \"content of the answer\"}}\n",
    "\n",
    "Strictly return results in the JSON format described above without any additional explanatory text.\n",
    "\n",
    "User Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "    # Get GPT analysis result\n",
    "    print(\"Analyzing query type...\")\n",
    "    analysis_result = json.loads(get_openai_response(prompt, \"json_object\"))\n",
    "    print(f\"Analysis Result: {json.dumps(analysis_result, ensure_ascii=False, indent=2)}\")\n",
    "\n",
    "    # Execute corresponding skills based on analysis result\n",
    "    if analysis_result['type_code'] == 'search':\n",
    "        print(\"\\nExecuting search skill\")\n",
    "        print(f\"Search keyword: {analysis_result['keyword']}\")\n",
    "        return search_and_process(analysis_result['question'])\n",
    "            \n",
    "    elif analysis_result['type_code'] == 'crawl':\n",
    "        print(\"\\nExecuting website crawling\")\n",
    "        crawl_url = analysis_result['url']\n",
    "        existing_urls = [crawl_url]\n",
    "        main_domain = extract_main_domain(crawl_url)\n",
    "        website_bot = Website_agent(main_domain, existing_urls)\n",
    "        result = website_bot.test_process_website(crawl_url, analysis_result['question'])\n",
    "        \n",
    "        print(\"Crawl result:\")\n",
    "        print(f\"Status code: {result['status']}\")\n",
    "        if result['status'] == 0:\n",
    "            print(\"Error information:\")\n",
    "        else:\n",
    "            if result['status'] == 2 or result['status'] == 1:\n",
    "                print(f\"Answer: {result['answer']}\")\n",
    "            if result['status'] == 3:\n",
    "                print(\"Unable to provide an answer\")\n",
    "            if result['status'] == 4:\n",
    "                print(\"Unable to provide an answer\")\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    else:  # type_code == 'other'\n",
    "        print(\"\\nExecuting direct answer\")\n",
    "        return analysis_result\n",
    "\n",
    "def build_official_site_prompt(search_results: list) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt for determining official website\n",
    "    Args:\n",
    "        search_results: List of Google search results\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Building prompt for official site identification ===\")\n",
    "    \n",
    "    prompt = \"\"\"You are a website classification expert. Your task is to identify the most likely official website from search results.\n",
    "    \n",
    "Rules:\n",
    "1. Official websites usually have the following characteristics:\n",
    "   - The domain name usually contains the company/brand name\n",
    "   - URL structure is simple, typically the root domain\n",
    "   - Website title usually includes the company/brand name\n",
    "   - Generally not links to social media, news sites, or third-party platforms\n",
    "\n",
    "2. Factors to consider (in order of importance):\n",
    "   a) Matching of the domain name with the company/brand name\n",
    "   b) Whether it's the brand's main domain (not a subdomain)\n",
    "   c) Credibility of the link\n",
    "   d) Official nature of the website description\n",
    "\n",
    "Analyze the following search results and return the most likely official website URL in JSON format.\n",
    "\n",
    "Search Results:\n",
    "\"\"\"\n",
    "    # Add search results to the prompt\n",
    "    print(f\"\\nAnalyzing the following search results:\")\n",
    "    for idx, result in enumerate(search_results, 1):\n",
    "        url = result.get('link', '')\n",
    "        title = result.get('title', '')\n",
    "        snippet = result.get('snippet', '')\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "{idx}. Website Info:\n",
    "   URL: {url}\n",
    "   Title: {title}\n",
    "   Description: {snippet}\n",
    "\"\"\"\n",
    "        print(f\"\\n{idx}. {url}\")\n",
    "        print(f\"   Title: {title}\")\n",
    "\n",
    "    prompt += \"\"\"\n",
    "Return the analysis result in the following JSON format:\n",
    "{\n",
    "    \"official_url\": \"Most likely official website URL\",\n",
    "    \"confidence\": \"High/Medium/Low\",\n",
    "    \"reason\": \"Brief explanation of why this choice was made\"\n",
    "}\n",
    "\n",
    "Note:\n",
    "- If no obvious official website is found, set confidence to \"Low\"\n",
    "- If multiple possible official websites are found, choose the most likely one\n",
    "\"\"\"\n",
    "    print(\"\\nPrompt construction completed\")\n",
    "    return prompt\n",
    "\n",
    "def process_search_results(search_results: dict, question: str) -> dict:\n",
    "    \"\"\"\n",
    "    Process Google search results to find the official website\n",
    "    Args:\n",
    "        search_results: Full search results from Google Serper\n",
    "        question: Original question\n",
    "    Returns:\n",
    "        dict: Result containing the official website URL\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Beginning to process search results ===\")\n",
    "    try:\n",
    "        if 'organic' not in search_results:\n",
    "            print(\"❌ Error: Search results format error\")\n",
    "            return {\"error\": \"Search results format error\"}\n",
    "\n",
    "        print(f\"Found {len(search_results['organic'])} search results\")\n",
    "        \n",
    "        # Build analysis prompt\n",
    "        print(\"\\nStep 1: Building analysis prompt\")\n",
    "        prompt = build_official_site_prompt(search_results['organic'][:5])  # Only analyze the top 5 results\n",
    "        \n",
    "        # Analyze results\n",
    "        print(\"\\nStep 2: Using AI to analyze search results\")\n",
    "        try:\n",
    "            result = json.loads(get_openai_response(prompt, \"json_object\"))\n",
    "            \n",
    "            print(f\"\\n=== AI Analysis Result ===\")\n",
    "            print(f\"✓ Identified Official Website: {result['official_url']}\")\n",
    "            print(f\"✓ Confidence: {result['confidence']}\")\n",
    "            print(f\"✓ Reason for Choice: {result['reason']}\")\n",
    "            \n",
    "            if result['official_url']:\n",
    "                if result['confidence'] == \"Low\":\n",
    "                    print(\"\\nWarning: Low confidence in the official website identification, but proceeding anyway\")\n",
    "                    \n",
    "                print(f\"\\nStep 3: Starting website crawling\")\n",
    "                print(f\"Target URL: {result['official_url']}\")\n",
    "                \n",
    "                # Instantiate Website_agent and perform crawling\n",
    "                crawl_url = result['official_url']\n",
    "                existing_urls = [crawl_url]\n",
    "                main_domain = extract_main_domain(crawl_url)\n",
    "                \n",
    "                agent = Website_agent(main_domain, existing_urls)\n",
    "                return agent.test_process_website(\n",
    "                    url=crawl_url,\n",
    "                    question=question\n",
    "                )\n",
    "            else:\n",
    "                print(\"\\n❌ Error: No credible official website found\")\n",
    "                return {\"error\": \"No credible official website found\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"\\n❌ Error: Analysis process failed\")\n",
    "            print(f\"Error information: {str(e)}\")\n",
    "            return {\"error\": f\"Error occurred during analysis: {str(e)}\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"\\n❌ Error: Failed to process search results\")\n",
    "        print(f\"Error information: {str(e)}\")\n",
    "        return {\"error\": f\"Error occurred while processing search results: {str(e)}\"}\n",
    "\n",
    "def search_and_process(query: str) -> dict:\n",
    "    \"\"\"Search and process results\"\"\"\n",
    "    print(\"\\n=== Starting search process ===\")\n",
    "    search_keyword = f\"{query.strip()} official website\"\n",
    "    print(f\"Search keyword: {search_keyword}\")\n",
    "    \n",
    "    headers = {\n",
    "        'X-API-KEY': serper_api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'q': search_keyword,\n",
    "        'gl': 'us',\n",
    "        'page': 0,\n",
    "        'num': 5\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nStep 1: Calling search API\")\n",
    "        response = requests.post(\n",
    "            \"https://google.serper.dev/search\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Search request successful\")\n",
    "            search_results = response.json()\n",
    "            \n",
    "            print(\"\\nStep 2: Processing search results\")\n",
    "            return process_search_results(search_results, query)\n",
    "        else:\n",
    "            print(f\"\\n❌ Error: Search request failed (Status code: {response.status_code})\")\n",
    "            return {\"error\": f\"Search request failed: {response.status_code}\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"\\n❌ Error: Search process failed\")\n",
    "        print(f\"Error information: {str(e)}\")\n",
    "        return {\"error\": f\"Error occurred during the search process: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f7531e-b5c7-4745-bd62-749d326a8e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting query processing: Percent company's main members and contact information\n",
      "Analyzing query type...\n",
      "Analysis Result: {\n",
      "  \"type_code\": \"search\",\n",
      "  \"keyword\": \"Percent company official website\",\n",
      "  \"question\": \"Percent company's main members and contact information\"\n",
      "}\n",
      "\n",
      "Executing search skill\n",
      "Search keyword: Percent company official website\n",
      "\n",
      "=== Starting search process ===\n",
      "Search keyword: Percent company's main members and contact information official website\n",
      "\n",
      "Step 1: Calling search API\n",
      "✓ Search request successful\n",
      "\n",
      "Step 2: Processing search results\n",
      "\n",
      "=== Beginning to process search results ===\n",
      "Found 5 search results\n",
      "\n",
      "Step 1: Building analysis prompt\n",
      "\n",
      "=== Building prompt for official site identification ===\n",
      "\n",
      "Analyzing the following search results:\n",
      "\n",
      "1. https://percent.com/contact/\n",
      "   Title: Contact - Percent\n",
      "\n",
      "2. https://percent.com/\n",
      "   Title: Percent — Private credit investing. Simplified.\n",
      "\n",
      "3. https://poweredbypercent.com/legal/\n",
      "   Title: Legal - Percent\n",
      "\n",
      "4. https://leadiq.com/c/percent/5a1d9da82300005e008d86a7\n",
      "   Title: Percent Company Overview, Contact Details & Competitors | LeadIQ\n",
      "\n",
      "5. https://www.there100.org/re100-members\n",
      "   Title: RE100 Members\n",
      "\n",
      "Prompt construction completed\n",
      "\n",
      "Step 2: Using AI to analyze search results\n",
      "\n",
      "=== AI Analysis Result ===\n",
      "✓ Identified Official Website: https://percent.com/\n",
      "✓ Confidence: High\n",
      "✓ Reason for Choice: The URL 'https://percent.com/' is the main domain containing the exact brand name 'Percent', has a simple structure, the title 'Percent — Private credit investing. Simplified.' indicates it is likely the official site, and the description suggests it offers core company services, pointing to a credible and official nature.\n",
      "\n",
      "Step 3: Starting website crawling\n",
      "Target URL: https://percent.com/\n",
      "\n",
      "=== Starting website analysis ===\n",
      "Initial URL: https://percent.com/\n",
      "Max exploration depth: 3\n",
      "\n",
      "Step 1: Fetching page content\n",
      "Fetching page: https://percent.com/ (Attempt 1)\n",
      "Page successfully fetched\n",
      "\n",
      "Step 2: Analyzing page content\n",
      "\n",
      "Processing website: https://percent.com/\n",
      "Analyzing page content...\n",
      "Page analysis completed, extracting results...\n",
      "\n",
      "Step 3: Processing analysis result\n",
      "Status code: 2\n",
      "→ Partial answer found, further analysis needed...\n",
      "\n",
      "Step 4: Analyzing related pages\n",
      "Found 2 related links\n",
      "\n",
      "Analyzing related link 1/2: https://percent.com/contact\n",
      "\n",
      "Fetching and processing page: https://percent.com/contact\n",
      "Fetching page: https://percent.com/contact (Attempt 1)\n",
      "Page successfully fetched\n",
      "→ Partial related information found\n",
      "\n",
      "Analyzing related link 2/2: https://percent.com/about-us\n",
      "\n",
      "Fetching and processing page: https://percent.com/about-us\n",
      "Fetching page: https://percent.com/about-us (Attempt 1)\n",
      "Page successfully fetched\n",
      "→ Partial related information found\n",
      "\n",
      "Consolidating all found information...\n",
      "Query question: Percent company's main members and contact information\n",
      "\n",
      "Final result:\n",
      "{'status': 2, 'answer': \"The website content does not provide specific details on the main members of Percent's company. For contact information, you can visit their contact page or social media platforms like [Twitter](https://twitter.com/investpercent), [LinkedIn](https://www.linkedin.com/company/percent-technologies/), or [Facebook](https://www.facebook.com/investpercent/), as mentioned in the site footer.\\nContact information for Percent includes the email address hello@percent.com and the phone number 646 876 5141. Their office is located at 909 Third Ave #968, New York, NY 10150. Social media channels include Twitter, LinkedIn, AngelList, Facebook, and Instagram, which can be visited directly from their website.\\nThe main members of Percent's management team include:\\n1. Gary Reifman - Chief Product Officer [LinkedIn](https://www.linkedin.com/in/garyreifman/).\\n2. Vadim Shteynberg - Chief Technology Officer [LinkedIn](https://www.linkedin.com/in/shteynberg/).\\n3. Charlie Lienau - Head of Corporate Strategy [LinkedIn](https://www.linkedin.com/in/charlielienau/).\\n4. Bina Shetty - Head of Client Solutions [LinkedIn](https://www.linkedin.com/in/bina-shetty-5bb4638/).\\n5. Nelson Chu - Founder & CEO [LinkedIn](https://www.linkedin.com/in/nelson-chu/).\\n6. Prath Reddy, CFA - President [LinkedIn](https://www.linkedin.com/in/prath/).\\n\\nThe Board of Directors includes members such as:\\n1. Nelson Chu - Board Director.\\n2. Navtej S. Nandra - Independent Director, more information available on [MarketScreener](https://www.marketscreener.com/business-leaders/Navtej-Nandra-06ZSJS-E/biography/).\\n3. Eddie Lee - General Partner, White Star Capital [LinkedIn](https://www.linkedin.com/in/edwinlee01/).\\n4. Urs Cete - Managing Partner, BDMI [LinkedIn](https://www.linkedin.com/in/urscete/).\\n5. Louis Rajczi - Partner, Forte Ventures [LinkedIn](https://www.linkedin.com/in/louis-rajczi-8646512/).\\n\\nFor contact information, you can visit Percent's [Contact Us page](https://percent.com/contact/).\"}\n"
     ]
    }
   ],
   "source": [
    "#query2 = \"https://www.percent.cn/ Percent company's main members and contact information\"\n",
    "query2 = \"Percent company's main members and contact information\"\n",
    "# query2 = \"What's the weather like today\"\n",
    "result2 = process_query(query2)\n",
    "print(f\"Query question: {query2}\")\n",
    "print(\"\\nFinal result:\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335cbcd-15a3-4b6f-9880-2bf02c38cad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
